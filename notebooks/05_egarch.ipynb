{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f014202",
   "metadata": {},
   "source": [
    "## Import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff492972",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\Financial-volatility-forecasting\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from model import (\n",
    "    load_features,\n",
    "    train_egarch,\n",
    "    forecast_egarch,\n",
    "    evaluate_models\n",
    ")\n",
    "from feature_engineering import engineer_features\n",
    "from data_ingestion import get_data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from config import VOL_WINDOW, VOL_TARGET_HORIZON, DATA_RAW, TEST_SIZE,RESULTS_DIR\n",
    "import matplotlib.pyplot as plt\n",
    "from data_ingestion import get_sentiment_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9b7493",
   "metadata": {},
   "source": [
    "## Load Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8631234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Data is outdated (Last date: 2026-01-23). Re-downloading...\n",
      "Downloading SPY from 1993-01-01 to 2026-01-24...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\Financial-volatility-forecasting\\.venv\\Lib\\site-packages\\yfinance\\scrapers\\history.py:201: Pandas4Warning: Timestamp.utcnow is deprecated and will be removed in a future version. Use Timestamp.now('UTC') instead.\n",
      "  dt_now = pd.Timestamp.utcnow()\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data saved to C:\\Users\\hp\\Financial-volatility-forecasting\\data\\raw\\SPY.csv\n"
     ]
    }
   ],
   "source": [
    "df = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e078ba89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Fetching multi-source FinBERT news sentiment...\n",
      "✅ 13 headlines | 3 sources\n",
      " News sentiment: 3 days\n",
      "\n",
      " Fetching Google Trends data...\n",
      " Google Trends: 93 days\n",
      "\n",
      " Creating daily volatility CSV...\n",
      "✅ daily_vol.csv created (8293 days)\n",
      "\n",
      " Combining sentiment sources...\n",
      "✅ Combined sentiment saved: 93 days\n"
     ]
    }
   ],
   "source": [
    "sentiment_df = get_sentiment_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c586bd",
   "metadata": {},
   "source": [
    "## feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad7b6447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Processed data saved. Range: 1993-03-02 to 2026-01-23\n"
     ]
    }
   ],
   "source": [
    "df = engineer_features(df, sentiment_df, VOL_WINDOW, VOL_TARGET_HORIZON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c52c3bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.DataFrame'>\n",
      "DatetimeIndex: 8282 entries, 1993-03-02 to 2026-01-23\n",
      "Data columns (total 18 columns):\n",
      " #   Column                       Non-Null Count  Dtype  \n",
      "---  ------                       --------------  -----  \n",
      " 0   Close                        8282 non-null   float64\n",
      " 1   High                         8282 non-null   float64\n",
      " 2   Low                          8282 non-null   float64\n",
      " 3   Open                         8282 non-null   float64\n",
      " 4   Volume                       8282 non-null   int64  \n",
      " 5   log_return                   8282 non-null   float64\n",
      " 6   volatility_20d               8282 non-null   float64\n",
      " 7   abs_return                   8282 non-null   float64\n",
      " 8   return_squared               8282 non-null   float64\n",
      " 9   lag_1                        8282 non-null   float64\n",
      " 10  lag_5                        8282 non-null   float64\n",
      " 11  lag_10                       8282 non-null   float64\n",
      " 12  lag_20                       8282 non-null   float64\n",
      " 13  rolling_abs_return_mean_20d  8282 non-null   float64\n",
      " 14  sentiment                    8282 non-null   float64\n",
      " 15  sentiment_lag1               8282 non-null   float64\n",
      " 16  sentiment_scaled             8282 non-null   float64\n",
      " 17  target_volatility            8262 non-null   float64\n",
      "dtypes: float64(17), int64(1)\n",
      "memory usage: 1.2 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d0931ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_features()\n",
    "returns = df[\"log_return\"]\n",
    "target = df[\"target_volatility\"]\n",
    "sentiment = df[\"sentiment_lag1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92c7970a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineered data shape: (8282, 18)\n",
      "Columns: ['Close', 'High', 'Low', 'Open', 'Volume', 'log_return', 'volatility_20d', 'abs_return', 'return_squared', 'lag_1', 'lag_5', 'lag_10', 'lag_20', 'rolling_abs_return_mean_20d', 'sentiment', 'sentiment_lag1', 'sentiment_scaled', 'target_volatility']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Engineered data shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fffe2b",
   "metadata": {},
   "source": [
    "## split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19ae40e",
   "metadata": {},
   "source": [
    "### Separate the historical data from the live data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c894108b",
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_df = df.dropna(subset=['target_volatility'])\n",
    "live_df = df.iloc[-20:] # These are the 20 rows with NaN targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ea85d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 80/20 split on ONLY the historical data for training and testing\n",
    "split_idx = int(len(df) * (1 - TEST_SIZE))\n",
    "\n",
    "returns_test = df[\"log_return\"].iloc[split_idx:]\n",
    "returns_train = df[\"log_return\"].iloc[:split_idx]\n",
    "target_train = df[\"target_volatility\"].iloc[:split_idx]\n",
    "target_test = df[\"target_volatility\"].iloc[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "577352fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train size: 6625, Test size: 1657\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nTrain size: {len(returns_train)}, Test size: {len(returns_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99e5327f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set NaNs: 20\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test Set NaNs: {target_test.isna().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4d114b",
   "metadata": {},
   "source": [
    "## Rolling egarch loop (parallel to GARCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Running Rolling EGARCH-X with sentiment...\n",
      "Progress: 0/1657\n",
      "Progress: 50/1657\n",
      "Progress: 100/1657\n",
      "Progress: 150/1657\n",
      "Progress: 200/1657\n",
      "Progress: 250/1657\n",
      "Progress: 300/1657\n",
      "Progress: 350/1657\n",
      "Progress: 400/1657\n",
      "Progress: 450/1657\n",
      "Progress: 500/1657\n",
      "Progress: 550/1657\n",
      "Progress: 600/1657\n",
      "Progress: 650/1657\n",
      "Progress: 700/1657\n",
      "Progress: 750/1657\n",
      "Progress: 800/1657\n",
      "Progress: 850/1657\n",
      "Progress: 900/1657\n",
      "Progress: 950/1657\n",
      "Progress: 1000/1657\n",
      "Progress: 1050/1657\n",
      "Progress: 1100/1657\n",
      "Progress: 1150/1657\n",
      "Progress: 1200/1657\n",
      "Progress: 1250/1657\n",
      "Progress: 1300/1657\n",
      "Progress: 1350/1657\n",
      "Progress: 1400/1657\n",
      "Progress: 1450/1657\n",
      "Progress: 1500/1657\n",
      "Progress: 1550/1657\n",
      "Progress: 1600/1657\n",
      "Progress: 1650/1657\n",
      " EGARCH forecasting complete\n"
     ]
    }
   ],
   "source": [
    "egarch_preds = []\n",
    "print(\"\\n Running Rolling EGARCH-X with sentiment...\")\n",
    "\n",
    "# Use the full history for the expanding window\n",
    "for t in range(len(returns_test)):\n",
    "    if t % 50 == 0:\n",
    "        print(f\"Progress: {t}/{len(returns_test)}\")\n",
    "\n",
    "    # 1. Training Window (Grows by 1 each day)\n",
    "    # This includes all data from 1993 up to the day BEFORE the test day\n",
    "    r_train = returns.iloc[:split_idx + t]\n",
    "    x_train = sentiment.iloc[:split_idx + t].values.reshape(-1,1)\n",
    "\n",
    "    # 2. Train Model\n",
    "    # Ensure train_egarch handles cases where the optimizer fails to converge\n",
    "    model = train_egarch(r_train, x_train)\n",
    "\n",
    "    # 3. Forecast for the NEXT period\n",
    "    # We use the sentiment available at the end of the training window\n",
    "    # to predict the volatility of the test period\n",
    "    x_next = sentiment.iloc[[split_idx + t]].values.reshape(-1, 1)\n",
    "    \n",
    "    try:\n",
    "        pred = forecast_egarch(model, x_next)\n",
    "        egarch_preds.append(pred)\n",
    "    except:\n",
    "        # Fallback to previous prediction if solver fails\n",
    "        egarch_preds.append(egarch_preds[-1] if egarch_preds else 0)\n",
    "\n",
    "print(\" EGARCH forecasting complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cd8675",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a66141e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Const       0.037256\n",
      "x0          0.094935\n",
      "omega       0.001009\n",
      "alpha[1]    0.159924\n",
      "gamma[1]   -0.142624\n",
      "beta[1]     0.970173\n",
      "Name: params, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(model.params)\n",
    "\n",
    "# const = Mean equation intercept (average return)\n",
    "# x0 = Sentiment impact (how much sentiment affects volatility)\n",
    "# omega = Baseline volatility(long-term average)\n",
    "# alpha[1] = shock persistence (how much new shocks impact volatility)\n",
    "# gamma[1] = leverage effect (In the stock market, bad news (negative returns) usually spikes volatility much harder than good news (positive returns) drops it)\n",
    "# beta[1] = volatility persistence (how much past volatility influences future volatility)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87d230f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " EGARCH Performance:\n",
      " RMSE: 0.0978\n",
      " MAE: 0.0572\n",
      " R-squared: 0.2744\n",
      "\n",
      " EGARCH Historical Performance (Excluding 2026 NaNs):\n",
      "  RMSE: 0.0978\n",
      "  MAE: 0.0572\n",
      "  R²: 0.2744\n",
      "\n",
      " LIVE FORECAST for the next 20 days: 0.1412\n"
     ]
    }
   ],
   "source": [
    "# Convert preds to a Series so we can align it with the index\n",
    "preds_series = pd.Series(egarch_preds, index=target_test.index)\n",
    "\n",
    "# Filter out the NaNs from the ground truth (the last 20 days)\n",
    "# This keeps only the rows where we have an actual target to compare against\n",
    "valid_mask = ~target_test.isna()\n",
    "\n",
    "y_true_filtered = target_test[valid_mask].values\n",
    "y_pred_filtered = preds_series[valid_mask].values\n",
    "\n",
    "# run the evaluation on the CLEAN data\n",
    "metrics = evaluate_models(\n",
    "    y_true=y_true_filtered,\n",
    "    y_pred=y_pred_filtered,\n",
    "    model_name=\"EGARCH\"\n",
    ")\n",
    "\n",
    "print(f\"\\n EGARCH Historical Performance (Excluding 2026 NaNs):\")\n",
    "print(f\"  RMSE: {metrics['rmse']:.4f}\")\n",
    "print(f\"  MAE: {metrics['mae']:.4f}\")\n",
    "print(f\"  R²: {metrics['R_squared']:.4f}\")\n",
    "\n",
    "# 4. Grab your REAL 2026 Forecast\n",
    "live_forecast = egarch_preds[-1]\n",
    "print(f\"\\n LIVE FORECAST for the next 20 days: {live_forecast:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba67993",
   "metadata": {},
   "source": [
    "## Save Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f670a8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Predictions saved with dates to C:\\Users\\hp\\Financial-volatility-forecasting\\results\\preds\\egarch_preds.csv\n"
     ]
    }
   ],
   "source": [
    "egarch_preds_df = pd.DataFrame({\n",
    "    \"actual_volatility\": target_test, # This will have NaNs for 2026\n",
    "    \"egarch_forecast\": egarch_preds,       # This will have numbers for 2026\n",
    "}, index=target_test.index)\n",
    "\n",
    "egarch_preds_df.to_csv(RESULTS_DIR / \"preds\" / \"egarch_preds.csv\", index=True)\n",
    "print(f\"\\n✅ Predictions saved with dates to {RESULTS_DIR / 'preds' / 'egarch_preds.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a635b5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1927713b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9123fa9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
